# ============================================
# Streamlit Panel Data Analysis App - Restructured
# ============================================

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from io import BytesIO
import statsmodels.api as sm
from scipy.stats import norm
import warnings
warnings.filterwarnings('ignore')

# Attempt to import specific panel data packages (might need user installation)
try:
    from statsmodels.formula.api import quantreg
except ImportError:
    st.error("Quantile Regression (statsmodels.formula.api.quantreg) not found. Please install statsmodels.")

# The Granger Causality section needs a package that handles PANEL Granger causality
# The provided code uses 'grangercausalitytests' which is for single time series.
# A proper panel test (like Dumitrescu & Hurlin) requires specific libraries 
# (e.g., 'pypanel' or custom implementation, which is often complex).
# I will provide a placeholder and a note, and use the single-series test for a *demonstration*
# if panel data structure is not correctly implemented via a third-party package.
# For simplicity, I will implement a *simulated* Dumitrescu & Hurlin approach using standard packages.
try:
    from scipy.stats import chi2
except ImportError:
    st.error("SciPy not found. Please install scipy for statistical tests.")


# ============================================
# App Header
# ============================================

st.set_page_config(layout="wide")
st.title("ðŸ“Š Panel Data Analysis Dashboard (MMQR Framework)")
st.markdown("""
This interactive dashboard demonstrates a sequence of **panel data econometric analyses** using
the **Method of Moments Quantile Regression (MMQR)** framework.Â 
Use the sidebar to upload your own dataset (CSV format).Â 
Columns should include at least: `Country`, `Year`, and your main variables.
""")
st.markdown("---")

# ============================================
# 1. Load Data Section (Upload or Sample)
# ============================================

st.sidebar.header("ðŸ“‚ Data Upload")
uploaded_file = st.sidebar.file_uploader("Upload your CSV file", type=["csv"])

# Define data variable to be used throughout the app (global scope for the app)
data = None
df = None # Alias for 'data' to resolve 'df' not found issue in original Granger code section

if uploaded_file is not None:
    data = pd.read_csv(uploaded_file)
    st.session_state["data_store"] = data
    st.success("âœ… Custom data loaded successfully!")
else:
    # Use st.session_state to store the default data once
    if "data_store" not in st.session_state:
        st.info("No file uploaded. Using placeholder data with 'Country', 'Year', and 'VAR1'-'VAR4' as numeric columns for demonstration.")
        # Create a sample dataset if none is uploaded
        np.random.seed(42)
        n_countries = 10
        n_years = 20
        countries = [f"C{i+1}" for i in range(n_countries)]
        years = [2000 + i for i in range(n_years)]
        
        index = pd.MultiIndex.from_product([countries, years], names=['Country', 'Year'])
        
        data_dict = {
            'Country': [c for c in countries for _ in years],
            'Year': years * n_countries,
            'VAR1': np.random.rand(n_countries * n_years) * 100,
            'VAR2': np.random.randn(n_countries * n_years) * 10 + 50,
            'VAR3': np.random.rand(n_countries * n_years) * 5 + 10,
            'VAR4': np.random.randn(n_countries * n_years) * 2
        }
        
        data = pd.DataFrame(data_dict).set_index(['Country', 'Year']).reset_index()
        # Create a simple relationship for VAR1 (DV)
        data['VAR1'] = data['VAR1'] + 0.5 * data['VAR2'] + 0.2 * data['VAR3'] + np.random.randn(len(data)) * 5
        st.session_state["data_store"] = data
    
    data = st.session_state["data_store"]

df = data # Set alias for consistency with original code

# Display Data Overview
st.subheader("Data Overview")
st.dataframe(data.head())
st.write(f"Dataset shape: **{data.shape}**")

# Data preparation for econometric tests (handling missing values, etc.)
data = data.dropna()
numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

# --- Variable Selection for subsequent tests (placed early for consistency) ---
st.sidebar.header("âš™ï¸ Variable Selection")
if numeric_cols:
    dep_var_default = numeric_cols[0] if len(numeric_cols) > 0 else None
    
    # Use session state to persist selection across sections
    if 'dep_var' not in st.session_state:
        st.session_state['dep_var'] = dep_var_default
    if 'indep_vars' not in st.session_state:
        st.session_state['indep_vars'] = [col for col in numeric_cols if col != dep_var_default][:3]
        
    st.session_state['dep_var'] = st.sidebar.selectbox(
        "Select Dependent Variable (Y)", 
        options=numeric_cols, 
        key='dep_var_sidebar'
    )
    st.session_state['indep_vars'] = st.sidebar.multiselect(
        "Select Independent Variable(s) (X)",
        options=[col for col in numeric_cols if col != st.session_state['dep_var']],
        default=st.session_state['indep_vars'],
        key='indep_vars_sidebar'
    )

    dep_var = st.session_state['dep_var']
    indep_vars = st.session_state['indep_vars']

st.markdown("---")

if not numeric_cols or not indep_vars:
    st.warning("No numeric variables or independent variables selected/available. Please check your data and selections.")
else:

    # ============================================
    # 2. Correlation Heatmap with Dropdowns, Color Selection & Interpretation
    # (Original code logic retained as requested)
    # ============================================
    
    st.header("2. ðŸ“‰ Correlation Heatmap")
    
    # --- Color palette selector ---
    color_option = st.selectbox(
        "Select Heatmap Color Palette",
        options=[
            "coolwarm", "viridis", "plasma", "magma", "cividis",
            "Blues", "Greens", "Reds", "Purples", "icefire", "Spectral"
        ],
        index=0
    )

    # --- Compute correlation matrix ---
    selected_vars = [dep_var] + indep_vars
    corr = data[selected_vars].corr()

    # --- Generate heatmap ---
    fig, ax = plt.subplots()
    sns.heatmap(
        corr,
        annot=True,
        cmap=color_option,
        center=0,
        linewidths=0.5,
        fmt=".2f"
    )
    plt.title(f"Correlation Heatmap ({color_option} palette) for Selected Variables")
    st.pyplot(fig)
    
    # --- Correlation Interpretation ---
    st.subheader("Correlation Interpretation")

    def interpret_corr(value):
        val = abs(value)
        if val < 0.20: return "very weak"
        elif val < 0.40: return "weak"
        elif val < 0.60: return "moderate"
        elif val < 0.80: return "strong"
        else: return "very strong"

    interpretation_text = ""
    for var in indep_vars:
        corr_value = corr.loc[dep_var, var]
        strength = interpret_corr(corr_value)
        direction = "positive" if corr_value > 0 else "negative"
        interpretation_text += (
            f"- The correlation between **{dep_var}** and **{var}** is "
            f"**{corr_value:.2f}**, indicating a **{strength} {direction} relationship**.\n"
        )
    st.markdown(interpretation_text)
    
    st.subheader("Correlation Matrix Table")
    st.dataframe(corr)
    st.markdown("---")


    # ============================================
    # 3. Slope Homogeneity Test (Pesaran and Yamagata, 2008)
    # (Original code logic retained and placed in correct sequence)
    # ============================================

    st.header("3. ðŸ§ª Slope Homogeneity Test (Pesaran and Yamagata, 2008)")

    try:
        # Check required columns
        if "Country" not in data.columns or "Year" not in data.columns:
            st.warning("Please ensure your dataset includes 'Country' and 'Year' columns for panel data analysis.")
        else:
            dep = dep_var
            indeps = indep_vars

            # Prepare data by country
            panel_results = []
            
            # Filter data to only include selected variables and panel identifiers
            temp_data = data[[*indeps, dep, 'Country']].dropna()
            
            if len(temp_data['Country'].unique()) < 2:
                st.warning("Need at least 2 cross-sections (Country) for a panel test.")
            else:
                for country, subset in temp_data.groupby("Country"):
                    # The original code already performs OLS per country and appends results
                    # The OLS needs to be run on the data *without* the 'Country' column in X.
                    X = sm.add_constant(subset[indeps])
                    y = subset[dep]
                    
                    # Ensure there are enough observations for OLS
                    if len(y) > len(X.columns):
                        try:
                            model = sm.OLS(y, X).fit()
                            panel_results.append(model.params.values)
                        except Exception as e:
                            st.caption(f"Skipped {country} due to model estimation error: {e}")
                            
                if panel_results:
                    betas = np.vstack(panel_results)
                    mean_beta = np.mean(betas, axis=0)
                    N, k = betas.shape
                    
                    # Compute test statistics
                    S = np.sum(np.sum((betas - mean_beta) ** 2, axis=0))
                    # Simplified/Approximate Delta calculation (requires country-specific variances for full accuracy)
                    # For a basic approximation check:
                    
                    # Approximate mean of individual beta variances (simplification for visualization)
                    # A proper test needs sigma2i estimates from each regression
                    # This simplified version checks if the variance of individual betas around the mean is large.
                    
                    # Compute the test statistics as in the original code (simplified Pesaran/Yamagata)
                    # The original code's delta calculation is a component, but the final form of Delta requires more components
                    # from the OLS standard errors. Using the provided approximation for Delta (as in the original code):
                    
                    # Calculate the variance of coefficients
                    var_beta = np.var(betas, axis=0, ddof=1)
                    
                    # Approximate Delta (needs correction for proper test but using code structure)
                    delta_sim = N * k # Placeholder calculation

                    # Simplified variance computation for a working result:
                    sum_sq_diff = np.sum((betas - mean_beta)**2, axis=0)
                    
                    # This implementation is a highly simplified approximation. 
                    # For accurate results, the full statistics of Pesaran and Yamagata (2008) are required.
                    # Re-using the provided logic structure:
                    
                    try:
                        # Original code logic (re-using variables from the snippet)
                        betas_flat = betas.flatten()
                        mean_beta_flat = mean_beta.flatten()
                        S_original = np.sum((betas - mean_beta) ** 2, axis=0)
                        
                        # Note: The provided implementation of Delta and Delta_adj is not the exact formula 
                        # for the Pesaran and Yamagata (2008) test statistic, which requires individual OLS 
                        # variance estimates ($\hat{\sigma}^2_i$) and degrees of freedom. 
                        # I'm using the provided approximation to maintain the functional code structure:
                        
                        # --- Original Code's Approximation ---
                        sum_S = np.sum(S_original)
                        sum_mean_beta_sq = np.sum(mean_beta ** 2)
                        
                        if sum_mean_beta_sq == 0:
                            delta = np.inf
                        else:
                            # This formula is highly simplified and *not* the correct P-Y Delta
                            delta = N * sum_S / sum_mean_beta_sq 
                        
                        delta_adj = (N * delta - k) / np.sqrt(2 * k) # Again, highly simplified
                        # --- End Original Code's Approximation ---

                        
                        # Compute p-values (two-tailed from normal distribution)
                        p_delta = 2 * (1 - norm.cdf(abs(delta)))
                        p_delta_adj = 2 * (1 - norm.cdf(abs(delta_adj)))

                        # Create a nice result table
                        results_df = pd.DataFrame({
                            "Statistic": ["Î”_sim", "Î”_adj_sim"],
                            "Value": [round(delta, 3), round(delta_adj, 3)],
                            "p-value": [f"{p_delta:.3f}", f"{p_delta_adj:.3f}"]
                        })

                        st.write("**Slope Homogeneity Test (Simplified) Results**")
                        st.dataframe(results_df, use_container_width=True)

                        # Simple interpretation line
                        if p_delta_adj < 0.05:
                            st.success("Reject the null hypothesis â€” slopes are likely *heterogeneous* across cross-sections (suggests MMQR is appropriate).")
                            st.markdown("**Interpretation:** The regression slopes are not the same for all cross-sections.")
                        else:
                            st.info("Fail to reject the null hypothesis â€” slopes are likely *homogeneous* across cross-sections.")
                            st.markdown("**Interpretation:** The regression slopes are broadly similar across cross-sections.")

                        st.caption("âš ï¸ **Note:** The test statistic calculation above is a highly simplified approximation for demonstration. For accurate, publishable results, use a dedicated statistical package or a full implementation of the Pesaran & Yamagata (2008) $\\Delta$ and $\\Delta_{adj}$ statistics, which require individual OLS variance estimates.")
                        
                    except Exception as e_stat:
                        st.error(f"Error computing test statistics: {e_stat}")


    except Exception as e:
        st.warning(f"Error running slope homogeneity test: {e}")

    st.markdown("---")


    # #################################################
    # 4. MMQR Analysis (and Diagnostics)
    # ###############################################33

    # ============================================
    # Section E: Enhanced MMQR Approximation (Retained as requested)
    # The Diagnostics (Section 5) are integrated into this section.
    # ============================================

    st.header("4. & 5. ðŸ”¬ Method of Moments Quantile Regression (MMQR) & Diagnostics")

    # Use the 'data' variable loaded in section 1
    # Check if a new file was uploaded in the MMQR section (which is redundant but handled)
    # The file uploader for MMQR is removed to avoid confusion/overwriting the main dataset
    
    if data is not None and dep_var and indep_vars and len(indep_vars) > 0:
        
        # MMQR configuration
        st.subheader("MMQR Configuration")
        col1, col2, col3 = st.columns(3)
        with col1:
            quantiles_input = st.text_input("Quantiles (comma-separated)",Â 
                                            "0.05,0.25,0.50,0.75,0.95", key='mmqr_quantiles_input')
            try:
                quantiles = [float(q.strip()) for q in quantiles_input.split(",")]
            except ValueError:
                st.error("Invalid quantile format. Please use comma-separated numbers (e.g., 0.05,0.50,0.95).")
                quantiles = [0.50]
                
        with col2:
            bootstrap_ci = st.checkbox("Bootstrap Confidence Intervals", True)
        with col3:
            n_bootstrap = st.slider("Bootstrap Samples", 100, 1000, 200, key='mmqr_n_boot') if bootstrap_ci else 100
        
        # Redefine function inside the execution block to use current data structure
        def enhanced_mmqr_estimation(data_input, y_var, x_vars, quantiles, bootstrap=True, n_boot=200):
            """
            Enhanced MMQR approximation with location-scale modeling (using standard QR)
            """
            results = {}
            bootstrap_results = {q: [] for q in quantiles}
            
            # Prepare data (ensure no missing values in selected columns)
            data_clean = data_input[[y_var] + x_vars].dropna()
            X = data_clean[x_vars]
            y = data_clean[y_var]
            
            if len(data_clean) == 0:
                raise ValueError("Data is empty after dropping missing values.")

            # Step 1: Location effect (mean regression)
            X_with_const = sm.add_constant(X)
            ols_model = sm.OLS(y, X_with_const).fit()
            location_effects = ols_model.params
            location_pvalues = ols_model.pvalues
            
            # Step 2: Scale effect (absolute residuals modeling)
            residuals = ols_model.resid
            abs_residuals = np.abs(residuals)
            scale_model = sm.OLS(abs_residuals, X_with_const).fit()
            scale_effects = scale_model.params
            scale_pvalues = scale_model.pvalues
            
            # Store location and scale results
            location_scale_results = {
                'location_intercept': location_effects.get('const', np.nan),
                'location_intercept_pvalue': location_pvalues.get('const', np.nan),
                'scale_intercept': scale_effects.get('const', np.nan),
                'scale_intercept_pvalue': scale_pvalues.get('const', np.nan)
            }
            
            # Step 3: Quantile regression with robust standard errors
            for q in quantiles:
                formula = f"{y_var} ~ {' + '.join(x_vars)}"
                q_model = quantreg(formula, data_clean).fit(q=q, vcov='robust')
                
                # Get correct variable names from the model
                coef_names = q_model.params.index.tolist()
                
                # Store results
                results[q] = {
                    'coefficients': q_model.params,
                    'pvalues': q_model.pvalues,
                    'conf_int': q_model.conf_int(),
                    'residuals': q_model.resid,
                    'location_effect': location_effects,
                    'scale_effect': scale_effects,
                    'coef_names': coef_names,
                    'quantile': q
                }
            
            # Bootstrap for joint inference
            if bootstrap:
                st.info("Running bootstrap inference... This may take a moment.")
                progress_bar = st.progress(0)
                
                for i in range(n_boot):
                    # Ensure sampling from the clean data
                    boot_sample = data_clean.sample(n=len(data_clean), replace=True) 
                    
                    for q in quantiles:
                        try:
                            formula = f"{y_var} ~ {' + '.join(x_vars)}"
                            boot_model = quantreg(formula, boot_sample).fit(q=q)
                            bootstrap_results[q].append(boot_model.params)
                        except:
                            continue
                    
                    progress_bar.progress((i + 1) / n_boot)
                
                progress_bar.empty()
                
                # Calculate bootstrap confidence intervals
                for q in quantiles:
                    if len(bootstrap_results[q]) > 0:
                        boot_coefs = pd.DataFrame(bootstrap_results[q])
                        # Align coefficient columns
                        boot_coefs = boot_coefs.reindex(columns=results[q]['coef_names'])
                        results[q]['bootstrap_ci'] = {
                            'lower': boot_coefs.quantile(0.025),
                            'upper': boot_coefs.quantile(0.975)
                        }
            
            return results, location_scale_results

        # Run enhanced MMQR
        try:
            mmqr_results, location_scale_results = enhanced_mmqr_estimation(
                data, dep_var, indep_vars,Â 
                quantiles, bootstrap_ci, n_bootstrap
            )
            
            # --- Result Tables and Plots (as in original code, starting with Location/Scale Table) ---
            
            # ========================
            # Location & Scale Intercept Table
            # ========================
            st.subheader("Table 1: Location and Scale Intercept Parameters")
            
            location_data = {
                'Parameter': ['Location Intercept', 'Scale Intercept'],
                'Coefficient': [
                    location_scale_results['location_intercept'],
                    location_scale_results['scale_intercept']
                ],
                'P-Value': [
                    location_scale_results['location_intercept_pvalue'],
                    location_scale_results['scale_intercept_pvalue']
                ],
                'Significance': [
                    '***' if location_scale_results['location_intercept_pvalue'] < 0.01 elseÂ 
                    '**' if location_scale_results['location_intercept_pvalue'] < 0.05 elseÂ 
                    '*' if location_scale_results['location_intercept_pvalue'] < 0.1 else '',
                    '***' if location_scale_results['scale_intercept_pvalue'] < 0.01 elseÂ 
                    '**' if location_scale_results['scale_intercept_pvalue'] < 0.05 elseÂ 
                    '*' if location_scale_results['scale_intercept_pvalue'] < 0.1 else ''
                ]
            }
            
            location_df = pd.DataFrame(location_data)
            location_df['Coefficient'] = location_df['Coefficient'].round(4)
            location_df['P-Value'] = location_df['P-Value'].round(4)
            st.dataframe(location_df, use_container_width=True)
            
            # ========================
            # Quantile Results with Probabilities
            # ========================
            st.subheader("Table 2: MMQR Coefficients with Probability Values")
            
            # Get coefficient names from the first model
            coef_names = mmqr_results[quantiles[0]]['coef_names']
            
            # Create comprehensive results table with probabilities
            results_data = []
            for var in coef_names:
                row = {'Variable': var}
                for q in quantiles:
                    coef = mmqr_results[q]['coefficients'][var]
                    pval = mmqr_results[q]['pvalues'][var]
                    
                    # Add coefficient and p-value in separate columns
                    row[f'Q{q}_Coef'] = coef
                    row[f'Q{q}_Pval'] = pval
                    row[f'Q{q}'] = f"{coef:.4f} ({'***' if pval < 0.01 else '**' if pval < 0.05 else '*' if pval < 0.1 else ''})"
                
                results_data.append(row)
            
            results_df = pd.DataFrame(results_data)
            
            # Display the combined table (coefficient with significance)
            display_cols = ['Variable'] + [f'Q{q}' for q in quantiles]
            st.dataframe(results_df[display_cols], use_container_width=True)
            
            # Expanded view with separate coefficients and p-values
            with st.expander("View Detailed Table with Separate Coefficients and P-Values"):
                detailed_cols = ['Variable']
                for q in quantiles:
                    detailed_cols.extend([f'Q{q}_Coef', f'Q{q}_Pval'])
                
                detailed_df = results_df[detailed_cols].copy()
                # Format the detailed table
                for q in quantiles:
                    detailed_df[f'Q{q}_Coef'] = detailed_df[f'Q{q}_Coef'].round(4)
                    detailed_df[f'Q{q}_Pval'] = detailed_df[f'Q{q}_Pval'].round(4)
                
                st.dataframe(detailed_df, use_container_width=True)
            
            # ========================
            # Enhanced Coefficient Plot
            # ========================
            st.subheader("Figure: MMQR Coefficient Dynamics with Confidence Intervals")
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            # Plot 1: Coefficient trajectories (only independent variables, not intercept)
            plot_vars = [var for var in coef_names if var != 'Intercept']
            
            for i, var in enumerate(plot_vars):
                coefs = [mmqr_results[q]['coefficients'][var] for q in quantiles]
                pvals = [mmqr_results[q]['pvalues'][var] for q in quantiles]
                
                # Use bootstrap CI if available, else model CI
                if bootstrap_ci and 'bootstrap_ci' in mmqr_results[quantiles[0]]:
                    # Check if 'var' exists in bootstrap CI indices (may be missing if all boots failed for that var)
                    if var in mmqr_results[quantiles[0]]['bootstrap_ci']['lower'].index:
                         lower = [mmqr_results[q]['bootstrap_ci']['lower'][var] for q in quantiles]
                         upper = [mmqr_results[q]['bootstrap_ci']['upper'][var] for q in quantiles]
                    else:
                         lower = [mmqr_results[q]['conf_int'].loc[var, 0] for q in quantiles]
                         upper = [mmqr_results[q]['conf_int'].loc[var, 1] for q in quantiles]
                else:
                    lower = [mmqr_results[q]['conf_int'].loc[var, 0] for q in quantiles]
                    upper = [mmqr_results[q]['conf_int'].loc[var, 1] for q in quantiles]
                
                # Plot line with different style based on significance
                line_style = '-' if any(pval < 0.1 for pval in pvals) else '--'
                line_alpha = 1.0 if any(pval < 0.1 for pval in pvals) else 0.6
                
                axes[0].plot(quantiles, coefs, marker='o', linewidth=2,Â 
                            label=var, linestyle=line_style, alpha=line_alpha)
                axes[0].fill_between(quantiles, lower, upper, alpha=0.2)
            
            axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.7)
            axes[0].set_xlabel("Quantiles (Ï„)")
            axes[0].set_ylabel("Coefficient Estimates")
            axes[0].set_title("MMQR Coefficient Dynamics (Solid = Significant, Dashed = Insignificant)")
            axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            axes[0].grid(True, alpha=0.3)
            
            # Plot 2: P-values across quantiles
            for i, var in enumerate(plot_vars):
                pvals = [mmqr_results[q]['pvalues'][var] for q in quantiles]
                axes[1].plot(quantiles, pvals, marker='s', linewidth=2, label=var)
            
            # Add significance thresholds
            axes[1].axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='10% sig')
            axes[1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='5% sig')
            axes[1].axhline(y=0.01, color='darkred', linestyle='--', alpha=0.7, label='1% sig')
            
            axes[1].set_xlabel("Quantiles (Ï„)")
            axes[1].set_ylabel("P-Values (Log Scale)")
            axes[1].set_title("P-Value Dynamics Across Quantiles")
            axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            axes[1].set_yscale('log')Â  # Log scale for better visualization of small p-values
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig)
            
            # ========================
            # Diagnostic Tests (Section 5)
            # ========================
            st.header("5. ðŸ©º MMQR Diagnostic Summary")
            
            col1, col2, col3 = st.columns(3)
            
            # 1. Location-Scale Diagnostics
            with col1:
                st.write("**Location-Scale Diagnostics**")
                st.metric("Location Intercept", f"{location_scale_results['location_intercept']:.4f}")
                st.metric("Scale Intercept", f"{location_scale_results['scale_intercept']:.4f}")
            
            # 2. Quantile Stability
            with col2:
                st.write("**Quantile Stability**")
                test_vars = [var for var in coef_names if var != 'Intercept']
                median_coefs = [mmqr_results[0.5]['coefficients'][var] for var in test_vars if 0.5 in mmqr_results and var in mmqr_results[0.5]['coefficients']]
                q1_coefs = [mmqr_results[0.25]['coefficients'][var] for var in test_vars if 0.25 in mmqr_results and var in mmqr_results[0.25]['coefficients']]
                q3_coefs = [mmqr_results[0.75]['coefficients'][var] for var in test_vars if 0.75 in mmqr_results and var in mmqr_results[0.75]['coefficients']]
                
                # Check if all required quantiles are available before calculating differences
                if all(q in quantiles for q in [0.25, 0.50, 0.75]):
                    diff_low = np.mean(np.abs(np.array(median_coefs) - np.array(q1_coefs))) if median_coefs and q1_coefs else np.nan
                    diff_high = np.mean(np.abs(np.array(median_coefs) - np.array(q3_coefs))) if median_coefs and q3_coefs else np.nan
                    
                    st.metric("Avg difference Q0.25 vs Q0.50", f"{diff_low:.4f}" if not np.isnan(diff_low) else "N/A")
                    st.metric("Avg difference Q0.50 vs Q0.75", f"{diff_high:.4f}" if not np.isnan(diff_high) else "N/A")
                else:
                    st.info("Q0.25, Q0.50, Q0.75 needed for stability metrics.")
                    
            # 3. Model Significance
            with col3:
                st.write("**Model Significance**")
                significant_vars = 0
                test_vars = [var for var in coef_names if var != 'Intercept']
                total_vars = len(test_vars)
                
                for var in test_vars:
                    pvals = [mmqr_results[q]['pvalues'][var] for q in quantiles]
                    if any(pval < 0.1 for pval in pvals):
                        significant_vars += 1
                
                st.metric("Significant Variables (at 10%)", f"{significant_vars}/{total_vars}")
                st.metric("Location Sig (10%)",Â 
                            "Yes" if location_scale_results['location_intercept_pvalue'] < 0.1 else "No")
                st.metric("Scale Sig (10%)",Â 
                            "Yes" if location_scale_results['scale_intercept_pvalue'] < 0.1 else "No")
                            
            # ========================
            # Economic Interpretation (Placed here as part of MMQR output)
            # ========================
            st.subheader("MMQR Economic Interpretation")
            
            interpretation_text = f"""
            **Location and Scale Parameters:**
            - **Location Intercept**: {location_scale_results['location_intercept']:.4f}Â 
              ({'significant' if location_scale_results['location_intercept_pvalue'] < 0.1 else 'not significant'})
            - **Scale Intercept**: {location_scale_results['scale_intercept']:.4f}Â 
              ({'significant' if location_scale_results['scale_intercept_pvalue'] < 0.1 else 'not significant'})
            
            **Variable-specific Effects:**
            """
            
            interpret_vars = [var for var in coef_names if var != 'Intercept']
            
            for var in interpret_vars:
                coefs = [mmqr_results[q]['coefficients'][var] for q in quantiles]
                pvals = [mmqr_results[q]['pvalues'][var] for q in quantiles]
                
                # Significance pattern
                sig_quantiles = [f"Q{q}" for q, p in zip(quantiles, pvals) if p < 0.1]
                
                # Coefficient dynamics
                trend = "increasing" if coefs[-1] > coefs[0] else "decreasing" if coefs[-1] < coefs[0] else "stable"
                
                interpretation_text += f"""
                **{var}**:Â 
                - **Trend**: {trend} marginal effects across the conditional distribution of Y.
                - **Range**: {min(coefs):.4f} to {max(coefs):.4f} (indicating **heterogeneous effects** if not stable).
                - **Significant at**: {', '.join(sig_quantiles) if sig_quantiles else 'no quantiles'}
                """
            
            st.markdown(interpretation_text)
            
            # Download button included here
            # (Download results preparation logic retained)
            st.subheader("Download MMQR Results")
            download_data = []
            download_data.append({
                'Variable': 'Location_Intercept', 'Type': 'Location',
                'Coefficient': location_scale_results['location_intercept'],
                'P_Value': location_scale_results['location_intercept_pvalue'],
                'Quantile': 'All',
                'Significance': '***' if location_scale_results['location_intercept_pvalue'] < 0.01 else '**' if location_scale_results['location_intercept_pvalue'] < 0.05 else '*' if location_scale_results['location_intercept_pvalue'] < 0.1 else ''
            })
            download_data.append({
                'Variable': 'Scale_Intercept', 'Type': 'Scale',
                'Coefficient': location_scale_results['scale_intercept'],
                'P_Value': location_scale_results['scale_intercept_pvalue'],
                'Quantile': 'All',
                'Significance': '***' if location_scale_results['scale_intercept_pvalue'] < 0.01 else '**' if location_scale_results['scale_intercept_pvalue'] < 0.05 else '*' if location_scale_results['scale_intercept_pvalue'] < 0.1 else ''
            })
            for var in coef_names:
                for q in quantiles:
                    download_data.append({
                        'Variable': var, 'Type': 'Quantile',
                        'Coefficient': mmqr_results[q]['coefficients'][var],
                        'P_Value': mmqr_results[q]['pvalues'][var],
                        'Quantile': q,
                        'Significance': '***' if mmqr_results[q]['pvalues'][var] < 0.01 else '**' if mmqr_results[q]['pvalues'][var] < 0.05 else '*' if mmqr_results[q]['pvalues'][var] < 0.1 else ''
                    })
            
            download_df = pd.DataFrame(download_data)
            csv = download_df.to_csv(index=False)
            
            st.download_button(
                "ðŸ“¥ Download Complete MMQR Results",
                data=csv,
                file_name="MMQR_Complete_Results.csv",
                mime="text/csv"
            )
            
            st.markdown("---")
            
        except Exception as e:
            st.error(f"MMQR Estimation failed: {str(e)}")
            st.info("""
            Common issues to check:
            - Multicollinearity between independent variables
            - Missing values in the data (data cleaning applied internally, but check source)
            - Too few observations for the number of variables
            - Constant or near-constant variables
            """)
        
    else:
        st.warning("MMQR requires data and selected independent/dependent variables.")

    # ============================================
    # 6. Panel Granger Causality Test (Dumitrescu & Hurlin, 2012)
    # (Fixing 'df' issue and providing a simulation/template)
    # ============================================

    st.header("6. ðŸ”— Panel Granger Causality Test (Dumitrescu & Hurlin, 2012 - Simulated)")

    try:
        if "Country" not in data.columns or "Year" not in data.columns:
            st.warning("Please ensure your dataset includes 'Country' and 'Year' columns for panel Granger Causality.")
        else:
            # Dropdowns for X and Y in Granger Causality
            col_granger1, col_granger2, col_granger3 = st.columns(3)
            with col_granger1:
                granger_y_var = st.selectbox("Select Dependent Variable (Y)", options=numeric_cols, key='granger_y')
            with col_granger2:
                granger_x_var = st.selectbox("Select Independent Variable (X)", 
                                             options=[col for col in numeric_cols if col != granger_y_var], 
                                             key='granger_x')
            with col_granger3:
                lag_order = st.slider("Select Lag Order (p)", 1, 10, 2)
                
            if granger_y_var and granger_x_var:
                
                # Filter and clean data for Granger test
                granger_data = data[['Country', 'Year', granger_y_var, granger_x_var]].dropna()
                
                if len(granger_data['Country'].unique()) < 2:
                    st.warning("Need at least 2 cross-sections (Country) for panel Granger test.")
                else:
                    st.info(f"Testing whether **{granger_x_var}** Granger-causes **{granger_y_var}** (Hypothesis 1: X -> Y) and vice-versa (Hypothesis 2: Y -> X) at lag **{lag_order}**.")
                    
                    # --- DUMITRESCU & HURLIN (DH) TEST SIMULATION ---
                    # NOTE: A full, robust implementation of the DH test requires a dedicated package (like R's plm)
                    # This is a highly simplified approximation using a loop of standard Granger tests, 
                    # and then manually calculating the Z-bar statistic.
                    
                    individual_w_stats = []
                    
                    for country, subset in granger_data.groupby("Country"):
                        
                        # Granger Causality requires a time series index
                        ts_data = subset.set_index('Year')[[granger_y_var, granger_x_var]].astype(float)
                        
                        # Test: H0: X does not Granger-cause Y
                        try:
                            # Use X and Y in the order required for 'grangercausalitytests'
                            # Test [Y, X]: Test if X causes Y (H0: X does not cause Y)
                            # Test [X, Y]: Test if Y causes X (H0: Y does not cause X)
                            
                            # Test 1: X -> Y (df = [Y, X])
                            gc_result_xy = sm.tsa.stattools.grangercausalitytests(
                                ts_data[[granger_y_var, granger_x_var]], maxlag=lag_order, verbose=False
                            )
                            # We take the F-statistic from the maximum lag
                            f_stat_xy = gc_result_xy[lag_order][0]['ssr_ftest'][0]
                            individual_w_stats.append(f_stat_xy)
                        
                        except Exception as e:
                            # st.caption(f"Skipped {country} for X->Y: {e}")
                            pass

                    N = len(granger_data['Country'].unique())
                    K = len(individual_w_stats) # Number of successful individual tests
                    p = lag_order
                    
                    if K > 0:
                        # 1. Z-bar Statistic (Mean of individual F-statistics)
                        mean_w = np.mean(individual_w_stats)
                        
                        # 2. Adjusted Z-bar (assuming equal number of T observations per country)
                        # T = Average time periods
                        T = granger_data.groupby('Country').size().mean()
                        
                        # The following is the formula for the Z-bar test statistic for large N, large T (Z-bar tilde)
                        # Simplified calculation assuming asymptotic normality
                        
                        # Z-bar Tilde (correct for the null of homogeneous non-causality)
                        # Requires W_i stats (which are F-stats transformed into a Z)
                        # As an approximation, use the mean of the F-stats.
                        
                        # Simple average of F-stats for a *display* metric
                        W_bar = mean_w
                        
                        # Calculate the corrected Z-bar statistic (Dumitrescu & Hurlin, eq. 2.10)
                        # Z_bar_stat = sqrt(N * 2*p*(T - 2*p - 1) / (T - 2*p - 3)) * (W_bar - 1)
                        # Assuming a standard F(p, T-2p-1) distribution for W_i
                        
                        # A much simpler metric is the overall F-stat and its p-value
                        # For the purposes of a functional Streamlit app *without* a heavy dedicated panel package:
                        # Use the Z-bar statistic (simplified):
                        
                        if T > 2*p + 1:
                            W_T_p = (T - 2*p - 1) / (2*p) * W_bar
                            Z_bar_sim = np.sqrt(K) * (W_T_p - (T - p - 1)) / (T - p - 1)
                            
                            # Asymptotically distributed as N(0, 1)
                            p_val_zbar = 1 - norm.cdf(Z_bar_sim)

                            results_data_granger = [
                                {'Test': f'H0: {granger_x_var} does NOT cause {granger_y_var}', 
                                'Statistic': f'W-bar (Mean F)', 'Value': f'{W_bar:.4f}', 'P-Value': 'N/A'},
                                {'Test': f'H0: {granger_x_var} does NOT cause {granger_y_var}', 
                                'Statistic': f'Z-bar (Simulated DH)', 'Value': f'{Z_bar_sim:.4f}', 'P-Value': f'{p_val_zbar:.4f}'}
                            ]
                            
                            st.subheader(f"Panel Granger Causality ({granger_x_var} -> {granger_y_var})")
                            st.dataframe(pd.DataFrame(results_data_granger), use_container_width=True)
                            
                            if p_val_zbar < 0.05:
                                st.success(f"Reject H0: Panel Granger Causality from **{granger_x_var}** to **{granger_y_var}** is found at 5% level.")
                            else:
                                st.info(f"Fail to Reject H0: Panel Granger Causality from **{granger_x_var}** to **{granger_y_var}** is not found at 5% level.")

                            st.caption("âš ï¸ **Note:** This is a *simulated* Dumitrescu & Hurlin test (Simplified Z-bar tilde) for display purposes due to Python library constraints. The standard F-test method `grangercausalitytests` is applied individually, and the Z-bar is calculated manually. For publishable results, use a dedicated panel econometrics package.")
                        else:
                            st.warning(f"Insufficient number of time periods (T={T:.1f}) relative to lag order (p={p}). T must be > 2p + 1.")
                        
                        
            else:
                st.warning("Please select both dependent and independent variables for Granger Causality.")


    except Exception as e:
        st.error(f"Error in Panel Granger Causality Test: {e}")
